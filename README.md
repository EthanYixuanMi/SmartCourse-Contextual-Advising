# Smartcourse Contextual Advising

[![arXiv](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/abs/xxxx.xxxxx)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](#)
[![License](https://img.shields.io/badge/License-MIT-green)](#license)

## ðŸ” Overview

SmartCourse is a university course management and AI-driven advising system that provides personalized course recommendations based on a student's transcript and degree plan. This repository is an enhanced version of our original course project, extended for research and evaluation purposes.

> This project is based on the original coursework project:  
> [CS-Course-Project-in-Wenzhou-Kean-University/CPS3320](https://github.com/EthanYixuanMi/CS-Course-Project-in-Wenzhou-Kean-University/tree/main/CPS3320)


## âœ¨ New in This Version

This research version expands the original project with:
- ðŸ“Š Experimental evaluation on 25 advising queries
- ðŸŽ¯ Novel contextual prompts integrating transcripts and four-year plans
- ðŸ§  Local LLM (via Ollama) for context-aware recommendations
- ðŸ“ Custom metrics: `PlanScore`, `PersonalScore`, `Lift`, and `Recall`
- ðŸ§ª Context ablation experiments comparing 4 modes (full, no transcript, no plan, question-only)


## ðŸ§  How It Works

SmartCourse supports three user roles:
![System Architecture](assets/SmartCourse_arch.png)  
*Fig. 1. SmartCourse system architecture. Users interact via CLI or Gradio GUI; recommendations are generated by a local LLM. This figure corresponds to Fig. 1 in our [paper](https://arxiv.org/abs/xxxx.xxxxx).*


- **Student**: Enroll/drop courses, view grades, and request AI suggestions
- **Instructor**: Assign grades and review student enrollment
- **Administrator**: Manage course catalog and switch LLM models


All interactions can happen through:
- âœ… Command-Line Interface (CLI)
- ðŸŒ Gradio-based GUI


The AI Advising Module uses structured prompts that combine:  
[Transcript]()  
[Degree Plan]()  
[Student Question]()  
to provide personalized suggestions via a local LLM (e.g., LLaMA3.1:8B through Ollama).

## ðŸ›  Installation

### ðŸ”§ Requirements
- Python 3.8+
- Gradio, Requests

```bash
pip install gradio requests
```

## ðŸ“ Project Structure

The following files are required to run the system end-to-end (via CLI or GUI):  
`data/account.txt`, `data/course_list.txt`, `data/cps_plan.txt`, `main_frame/main.py`, and `main_frame/ui_gradio.py`.  

â”œâ”€â”€ assets/                  # All figures used in the paper (system architecture, GUI, results)  
â”œâ”€â”€ data/                    # Sample input data  
â”‚   â”œâ”€â”€ account.txt  
â”‚   â”œâ”€â”€ course_list.txt  
â”‚   â”œâ”€â”€ cps_plan.txt  
â”‚   â””â”€â”€ evaluation_questions.txt  
â”œâ”€â”€ experiment/              # Evaluation script for computing relevance metrics  
â”‚   â””â”€â”€ eval_relevance.py  
â”œâ”€â”€ main_frame/              # Main application logic  
â”‚   â”œâ”€â”€ course_manager.py  
â”‚   â”œâ”€â”€ data_models.py  
â”‚   â”œâ”€â”€ main.py              # CLI entry point  
â”‚   â”œâ”€â”€ ui_gradio.py         # Gradio-based GUI  
â”‚   â””â”€â”€ utils.py  
â”œâ”€â”€ results/                 # Experiment results  
â”‚   â””â”€â”€ relevance_scores.csv  
â”œâ”€â”€ LICENSE  
â””â”€â”€ README.md  
  



## â–¶ï¸ Running the App
CLI:
```bash
python main.py
```

GUI (Gradio):
```bash
python ui_gradio.py
```


## AI Integration (via Ollama)

![AI Advising Example](assets/ai_advice.png)  
*Fig. 2. Sample LLM response based on transcript and degree plan. This figure corresponds to Fig. 2 in our [paper](https://arxiv.org/abs/xxxx.xxxxx).*  

1. Install [Ollama](https://ollama.com/download)
2. Set model path as environment variable (for Windows):
```bash
setx OLLAMA_MODELS "D:\ollama_models" /M
```
3. Pull your model:
```bash
ollama pull llama3.1:8b
```

> ### ðŸ’¡ Need help installing Ollama?
> Try our visual installer: [Ollama Quick Installer for Windows](https://github.com/EthanYixuanMi/Ollama-Windows-Installer)


## ðŸ“Š Experimental Results

We evaluated SmartCourse on 25 representative advising queries across four context conditions, as described in our [paper](https://arxiv.org/abs/xxxx.xxxxx). The table below corresponds to Table I.

| Mode          | #Rec | PlanScore | PersonalScore | Lift | Recall | Latency (s) |
| ------------- |------|-----------|----------------|------|--------|-------------|
| Full Context  | **6.56** | 0.53      | **0.78**       | **0.25** | 0.15   | 47.65       |
| No Plan       | 2.24 | 0.03      | 0.19           | 0.16 | 0.01   | 25.36       |
| No Transcript | 6.20 | **0.60**  | 0.69           | 0.09 | **0.17** | 34.34       |
| Question Only | 0.04 | 0.04      | 0.04           | 0.00 | 0.00   | 21.52       |


![Context Evaluation Comparison](assets/bar_chart.png)  
*Fig. 3. Evaluation metrics (PlanScore, PersonalScore, Lift, Recall) across four context modes. This figure corresponds to Fig. 6 in our [paper](https://arxiv.org/abs/xxxx.xxxxx).*


## ðŸ“„ Citation
Our citation will be released once the arxiv link are available.







SmartCourse demonstrates how contextual LLMs can transform academic advising from static guidance to personalized planning.


## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---




